name=AI Commits

settings.title=Settings
settings.general.group.title=AI Commits
settings.locale=Locale
settings.locale.contextHelp=Listed locales appear in the IDE's language. Prompts use English locales (e.g., German, French, etc.).
settings.prompt=Prompt
settings.github-star=Star on GitHub
settings.report-bug=Report bug
settings.kofi=Buy me a coffee
settings.github-sponsors=Sponsor me
settings.verifyToken=Verify
settings.verify.valid=Configuration is valid.
settings.verify.invalid=Invalid configuration: {0}.
settings.verify.running=Verifying configuration...
settings.refreshModels.success=Models have been refreshed successfully.
settings.refreshModels.running=Refreshing models....
settings.refreshModels=Refresh
unknown-error=Unknown error

action.background=Generating a commit message
action.error=Commit message could not be generated.
action.unknown-error=An unknown error has occurred.
action.tooltip=Generate Commit Message with {0}

notifications.title=AI Commits
notifications.welcome=Thanks for installing AI Commits <strong>{0}</strong>
notifications.unsuccessful-request=Error occurred: {0}
notifications.internal-server-error=Internal server error ({0}) occurred.
notifications.unknown-error=Unknown error ({0}).
notification.group.important.name=AI Commits important
notification.group.general.name=AI Commits general
notifications.empty-diff=Git diff is empty.
notifications.no-common-branch=Branch could not be found. Replacing {branch} with main.
notifications.client-not-set=LLM client is not set.

actions.do-not-ask-again=Do not ask me again.
actions.take-me-there=Take me there.
actions.sure-take-me-there=Sure, take me there.
notifications.prompt-too-large=The diff is too large for the OpenAI API. Try reducing the number of staged changes, or write your own commit message.
notifications.no-commit-message=Commit field has not been initialized correctly.
notifications.unable-to-save-token=Token could not be saved: {0}.
settings.addPrompt=Add prompt
settings.prompt.name=Name
settings.prompt.content=Content
validation.required=This value is required.
validation.integer=Value should be an integer.
validation.float=Value should be a float.
validation.double=Value should be double.
validation.temperature=The temperature should be between 0 and 2.
settings.prompt.comment=<ul>\
  <li>Customize your prompt with variables: {locale}, {diff}, {branch}, {hint}, {taskId}, {taskSummary}, {taskDescription} and {taskTimeSpent}.</li>\
  <li>Include a hint by using a dollar sign prefix like this: {Here is a hint: $hint}. This adds the hint text inside the curly brackets only if a hint is provided.</li>\
  <li>Note: The prompt preview displays only the first 10,000 characters.</li>\
 </ul>
actions.update=Update
actions.add=Add
settings.prompt.edit.title=Edit Prompt
settings.prompt.add.title=Add Prompt
settings.prompt.description=Description
settings.prompt.projectSpecific=Project specific
settings.prompt.projectSpecific.contextHelp=When selected, saves the chosen prompt and locale for this project only.
validation.unique=Value already exists.
settings.loadingModels=Loading OpenAI models...
settings.more-prompts=More prompts
settings.exclusions.group.title=Exclusions
settings.exclusions.app.exclusion=Exclusion glob
settings.exclusions.app.dialog.label=Enter the new path glob ('*' and '?' allowed):
settings.exclusions.app.dialog.title=Add Exclusion Glob
settings.exclusions.app.title=Global exclusions
settings.exclusions.project.title=Project exclusions
settings.prompt.hint=Hint
settings.prompt.hint.comment=Note: This field is for testing purposes only. When generating the actual prompt, the {hint} variable will be replaced with content from the commit dialog.
settings.llmClient=LLM Client
settings.llmClient.projectSpecific=Project specific
settings.llmClient.projectSpecific.contextHelp=When selected, saves the chosen LLM client for this project only.
settings.llmClient.name=Name
settings.llmClient.host=Host
settings.llmClient.proxy=Proxy URL
settings.llmClient.proxy.comment=Creates an HTTP proxy from url.
settings.llmClient.token=Token
settings.llmClient.token.stored=<hidden>
settings.llmClient.modelId=Model
settings.llmClient.modelId.comment=This is an editable combo box, which means you can write your own model IDs, and they will be added to the dropdown.
settings.llmClient.timeout=Timeout
settings.llmClient.temperature=Temperature
settings.llmClient.temperature.comment=What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make \
the output more random, while lower values like 0.2 will make it more focused and deterministic.
settings.llmClient.streamingResponse=Streaming response
settings.llmClient.streamingResponse.contextHelp=Some models do not support streaming response and will fall back to normal response.
settings.llmClient.topK=Top K
settings.llmClient.topK.comment=The Top K parameter changes how the model selects tokens for output. \
  A Top K of 1 means the selected token is the most probable among all the tokens in the model's vocabulary (also called greedy decoding), \
  while a Top K of 3 means that the next token is selected from among the 3 most probable using the temperature. \
  For each token selection step, the Top K tokens with the highest probabilities are sampled. \
  Tokens are then further filtered based on topP with the final token selected using temperature sampling.
settings.llmClient.topP=Top P
settings.llmClient.topP.comment=The Top P parameter changes how the model selects tokens for output. \
  Tokens are selected from the most to least probable until the sum of their probabilities equals the Top P value. \
  For example, if tokens A, B, and C have a probability of 0.3, 0.2, and 0.1 and the Top P value is 0.5, \
  then the model will select either A or B as the next token by using the temperature and exclude C as a candidate.

settings.openAI.token.example=sk-ABCdefgHIjKlxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
settings.openAi.token.comment=You can get your token <a href="https://platform.openai.com/account/api-keys">here.</a>
settings.openAi.organizationId=Organization ID

settings.ollama.numCtx=Num ctx
settings.ollama.numCtx.comment=This controls how many tokens the LLM can use as context to generate the next token.
settings.ollama.numPredict=Num predict
settings.ollama.numPredict.comment=This parameter tells the LLM the maximum number of tokens it is allowed to generate.

settings.qianfan.apiKey=API key
settings.qianfan.secretKey=Secret key

settings.gemini.project-id=Project ID
settings.gemini.project-id.comment=Google Cloud project's ID.
settings.gemini.location=Location

settings.geminiGoogle.token=API key
settings.geminiGoogle.token.comment=You can get your token <a href="https://aistudio.google.com/app/u/2/apikey">here.</a>.
settings.geminiGoogle.token.example=BZcxxx-xxxx-xxxxx-xxxxxxxxxxxxxxxxxxxxx

settings.githubModels.token.example=github_pat_34********************************************************************************
settings.githubModels.token.comment=You can get your token <a href="https://github.com/settings/tokens">here.</a>

settings.anthropic.token.example=sk-ant-api03-TTz_qsxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
settings.anthropic.token.comment=You can get your token <a href="https://console.anthropic.com/settings/keys">here.</a>
settings.anthropic.version=Version
settings.anthropic.version.comment=Version of the anthropic API.
settings.anthropic.beta=Beta
settings.anthropic.beta.comment=Leave empty if you're not sure what this is about.

settings.azureOpenAi.host=Endpoint
settings.azureOpenAi.modelId=Deployment name
settings.azureOpenAi.token=Key
settings.azureOpenAi.token.example=a3cxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
settings.azureOpenAi.token.comment=This value can be found in the Keys and Endpoint section when examining your resource from the Azure portal. You can use either KEY1 or KEY2.

settings.huggingface.token.comment=HuggingFace access token can be generated <a href="https://huggingface.co/settings/tokens">here</a>.
settings.huggingface.token.example=hf_fKASPPYLkasgjasKwpSnAASRdasdCdAsddsASSDF
settings.huggingface.maxNewTokens=Max new tokens
settings.huggingface.waitForModel=Wait for model
settings.huggingface.waitModel.comment=When a model is warm, it is ready to be used, and you will get a response relatively quickly. However, some models are cold and need to be loaded before they can be used. In that case, you will get a 503 error. Rather than doing many requests until it is loaded, you can wait for the model to be loaded.
settings.huggingface.removePrompt=Remove prompt
settings.huggingface.removePrompt.comment=Some models return the result prefixed with prompt. Checking this option will remove the prompt from the result.

settings.mistral.token.comment=You can get your token <a href="https://console.mistral.ai/api-keys">here.</a>
settings.mistral.token.example=DCjxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
settings.mistral.maxTokens=Max tokens

settings.amazonBedrock.accessKey=Access key
settings.amazonBedrock.accessKeyId.example=AKIAIOSFODNN7EXAMPLE
settings.amazonBedrock.accessKey.example=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
settings.amazonBedrock.accessKey.comment=You can get your credentials here: <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-api.html">here.</a>
settings.amazonBedrock.accessKeyId=Access key ID
settings.amazonBedrock.maxOutputTokens=Max output tokens
settings.amazonBedrock.region=Region
settings.amazonBedrock.modelId.comment=Models IDs can be found <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html">here</a>.
settings.amazonBedrock.credentialsProvider=Credentials provider:
settings.amazonBedrock.defaultCredentialsProvider=Default
settings.amazonBedrock.staticCredentialsProvider=Static
settings.amazonBedrock.profileName=Profile name
settings.amazonBedrock.defaultCredentialsProvider.comment=Use Default for discovering credentials from the host's environment and Static for a fixed set of credentials.

settings.claudeCode.cliPath=CLI Path
settings.claudeCode.cliPath.comment=Path to the Claude Code CLI executable. Leave empty to auto-detect from PATH.
settings.claudeCode.browse=Browse...
settings.claudeCode.detectPath=Detect
settings.claudeCode.timeout.comment=Timeout in seconds for CLI execution.
settings.claudeCode.model=Model
settings.claudeCode.model.comment=Model alias (sonnet, opus, haiku) or full name. Leave empty to use CLI default.
claudeCode.cliNotFound=Claude Code CLI not found. Please install it from https://claude.ai/download or specify the path manually.
claudeCode.pathNotConfigured=Claude Code CLI path is not configured. Please specify the path in settings.
claudeCode.pathNotFound=Claude Code CLI not found at path: {0}. Please verify the path is correct and the file is executable.
claudeCode.timeout=Claude Code CLI execution timed out.
