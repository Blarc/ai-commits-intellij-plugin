name=AI Commits

settings.title=Settings
settings.general.group.title=AI Commits
settings.locale=Locale
settings.prompt=Prompt
settings.report-bug=Report bug
settings.verifyToken=Verify
settings.verify.valid=Configuration is valid.
settings.verify.invalid=Invalid configuration: {0}.
settings.verify.running=Verifying configuration...
settings.refreshModels=Refresh
unknown-error=Unknown error

action.background=Generating commit message
action.error=Commit message could not be generated.
action.unknown-error=Unknown error has occurred.

notifications.title=AI Commits
notifications.welcome=Thanks for installing AI Commits <strong>{0}</strong>
notifications.unsuccessful-request=Error occurred: {0}
notifications.internal-server-error=Internal server error ({0}) occurred.
notifications.unknown-error=Unknown error ({0}).
notification.group.important.name=AI Commits important
notification.group.general.name=AI Commits general
notifications.empty-diff=Git diff is empty.
notifications.no-common-branch=Branch could not be found. Replacing {branch} with main.
notifications.client-not-set=LLM client is not set.

actions.do-not-ask-again=Do not ask me again.
actions.take-me-there=Take me there.
actions.sure-take-me-there=Sure, take me there.
notifications.prompt-too-large=The diff is too large for the OpenAI API. Try reducing the number of staged changes, or write your own commit message.
notifications.no-commit-message=Commit field has not been initialized correctly.
notifications.unable-to-save-token=Token could not be saved: {0}.
settings.addPrompt=Add prompt
settings.prompt.name=Name
settings.prompt.content=Content
validation.required=This value is required.
validation.integer=Value should be an integer.
validation.double=Value should be double.
validation.temperature=Temperature should be between 0 and 2.
settings.prompt.comment=<ul>\
  <li>Customize your prompt with variables: {locale}, {diff}, {branch}, {hint}, {taskId}, {taskSummary} and {taskDescription}.</li>\
  <li>Include a hint by using a dollar sign prefix like this: {Here is a hint: $hint}. This adds the hint text inside the curly brackets only if a hint is provided.</li>\
  <li>Note: The prompt preview displays only the first 10,000 characters.</li>\
 </ul>
actions.update=Update
actions.add=Add
settings.prompt.edit.title=Edit Prompt
settings.prompt.add.title=Add Prompt
settings.prompt.description=Description
settings.prompt.projectSpecific=Project specific
settings.prompt.projectSpecific.contextHelp=When selected, saves the chosen prompt and locale for this project only.
validation.unique=Value already exists.
settings.loadingModels=Loading OpenAI models...
settings.more-prompts=More prompts
settings.exclusions.group.title=Exclusions
settings.exclusions.app.exclusion=Exclusion glob
settings.exclusions.app.dialog.label=Enter new path glob ('*' and '?' allowed):
settings.exclusions.app.dialog.title=Add Exclusion Glob
settings.exclusions.app.title=Global exclusions
settings.exclusions.project.title=Project exclusions
settings.prompt.hint=Hint
settings.prompt.hint.comment=Note: This field is for testing purposes only. When generating the actual prompt, the {hint} variable will be replaced with content from the commit dialog.
settings.llmClient=LLM Client
settings.llmClient.projectSpecific=Project specific
settings.llmClient.projectSpecific.contextHelp=When selected, saves the chosen LLM client for this project only.
settings.llmClient.name=Name
settings.llmClient.host=Host
settings.llmClient.proxy=Proxy URL
settings.llmClient.proxy.comment=Creates an HTTP proxy from url.
settings.llmClient.token=Token
settings.llmClient.token.stored=<hidden>
settings.llmClient.modelId=Model
settings.llmClient.modelId.comment=This is an editable combo box, which means you can write your own model IDs, and they will be added to the dropdown.
settings.llmClient.timeout=Timeout
settings.llmClient.temperature=Temperature
settings.llmClient.temperature.comment=What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make \
the output more random,while lower values like 0.2 will make it more focused and deterministic.
settings.llmClient.streamingResponse=Streaming response
settings.llmClient.streamingResponse.contextHelp=Some models do not support streaming response and will fall back to normal response.


settings.openAI.token.example=sk-ABCdefgHIjKlxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
settings.openAi.token.comment=You can get your token <a href="https://platform.openai.com/account/api-keys">here.</a>
settings.openAi.organizationId=Organization ID

settings.qianfan.apiKey=API key
settings.qianfan.secretKey=Secret key

settings.gemini.project-id=Project ID
settings.gemini.project-id.comment=Google Cloud project's ID.
settings.gemini.location=Location

settings.geminiGoogle.token=API key
settings.geminiGoogle.token.comment=You can get your token <a href="https://aistudio.google.com/app/u/2/apikey">here.</a>.
settings.geminiGoogle.token.example=BZcxxx-xxxx-xxxxx-xxxxxxxxxxxxxxxxxxxxx
settings.geminiGoogle.topK=Top K
settings.geminiGoogle.topK.comment=The Top K parameter changes how the model selects tokens for output.\
  A Top K of 1 means the selected token is the most probable among all the tokens in the model's vocabulary (also called greedy decoding),\
  while a Top K of 3 means that the next token is selected from among the 3 most probable using the temperature.\
  For each token selection step, the Top K tokens with the highest probabilities are sampled.\
  Tokens are then further filtered based on topP with the final token selected using temperature sampling.
settings.geminiGoogle.topP=Top P
settings.geminiGoogle.topP.comment=The Top P parameter changes how the model selects tokens for output.\
  Tokens are selected from the most to least probable until the sum of their probabilities equals the Top P value.\
  For example, if tokens A, B, and C have a probability of 0.3, 0.2, and 0.1 and the Top P value is 0.5,\
  then the model will select either A or B as the next token by using the temperature and exclude C as a candidate.

settings.githubModels.topP=Top P
settings.githubModels.token.example=github_pat_34********************************************************************************
settings.githubModels.token.comment=You can get your token <a href="https://github.com/settings/tokens">here.</a>
settings.githubModels.topP.comment=The Top P parameter changes how the model selects tokens for output.\
  Tokens are selected from the most to least probable until the sum of their probabilities equals the Top P value.\
  For example, if tokens A, B, and C have a probability of 0.3, 0.2, and 0.1 and the Top P value is 0.5,\
  then the model will select either A or B as the next token by using the temperature and exclude C as a candidate.

settings.anthropic.token.example=sk-ant-api03-TTz_qsxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
settings.anthropic.token.comment=You can get your token <a href="https://console.anthropic.com/settings/keys">here.</a>
settings.anthropic.version=Version
settings.anthropic.version.comment=Version of the anthropic API.
settings.anthropic.beta=Beta
settings.anthropic.beta.comment=Leave empty if you're not sure what this is about.

settings.azureOpenAi.host=Endpoint
settings.azureOpenAi.modelId=Deployment name
settings.azureOpenAi.token=Key
settings.azureOpenAi.token.example=a3cxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
settings.azureOpenAi.token.comment=This value can be found in the Keys and Endpoint section when examining your resource from the Azure portal. You can use either KEY1 or KEY2.

settings.huggingface.token.comment=HuggingFace access token can be generated <a href="https://huggingface.co/settings/tokens">here</a>.
settings.huggingface.token.example=hf_fKASPPYLkasgjasKwpSnAASRdasdCdAsddsASSDF
settings.huggingface.maxNewTokens=Max new tokens
settings.huggingface.waitForModel=Wait for model
settings.huggingface.waitModel.comment=When a model is warm, it is ready to be used, and you will get a response relatively quickly. However, some models are cold and need to be loaded before they can be used. In that case, you will get a 503 error. Rather than doing many requests until it is loaded, you can wait for the model to be loaded.

